{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0293c111297a61481508202fcd690d673b0775ece2d2d867b62b8842b676a9a30",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AutoEncoders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Downloading the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\\n!unzip ml-1m.zip\\n!ls\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "###ML-100K\n",
    "'''\n",
    "!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "!unzip ml-100k.zip\n",
    "!ls\n",
    "'''\n",
    "###ML-1M\"\"\"\n",
    "'''\n",
    "!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "!unzip ml-1m.zip\n",
    "!ls\n",
    "'''"
   ]
  },
  {
   "source": [
    "## Importing the libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "source": [
    "## Importing the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using this dataset.\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")"
   ]
  },
  {
   "source": [
    "## Preparing the training set and the test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = \"int\")\n",
    "test_set = pd.read_csv(\"ml-100k/u1.test\", delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = \"int\")"
   ]
  },
  {
   "source": [
    "## Getting the number of users and movies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
   ]
  },
  {
   "source": [
    "## Converting the data into an array with users in lines and movies in columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings # id_movies start from 1, array index start from 0\n",
    "        new_data.append(list(ratings)) # convert np array to list\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "source": [
    "## Converting the data into Torch tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "source": [
    "## Creating the architecture of the Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module): # stacked autoencoder, inherited from nn.Module\n",
    "    def __init__(self, ): # ',' for parameters from inheritance\n",
    "        super(SAE, self).__init__() # get all classes and modules from inheritance\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # First full connection layer (fcl - encoder)\n",
    "        #                        |       |___ Number of nodes in first hidden layer (experiment with different numbers)\n",
    "        #                        |___________ Number of nodes in first fully connected layer\n",
    "        self.fc2 = nn.Linear(20, 10) # Second full connection layer (fc2 - encoder)\n",
    "        #                    |    |___ Number of nodes in second hidden layer (experiment with different numbers)\n",
    "        #                    |________ Number of nodes in first hidden layer\n",
    "        self.fc3 = nn.Linear(10, 20) # Third full connection layer (fc3 - decoder)\n",
    "        #                    |   |____ Number of nodes in second hidden layer (experiment with different numbers)\n",
    "        #                    |________ Number of nodes in third hidden layer\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # Fourth full connection layer (fc4 - decoder)\n",
    "        #                    |   |___________ Number of nodes in fourth hidden layer (experiment with different numbers)\n",
    "        #                    |_______________ Number of nodes in third hidden layer\n",
    "        self.activation = nn.Sigmoid() # sigmoid activation functions(try sigmoid rectifier activation functions)\n",
    "\n",
    "    def forward(self, x): # x : input vector\n",
    "        x = self.activation(self.fc1(x)) # get activation of first hidden layer (from input vector), encoder\n",
    "        x = self.activation(self.fc2(x)) # get activation of second hidden layer1, encoder\n",
    "        x = self.activation(self.fc3(x)) # get activation of third hidden layer, decoder\n",
    "        x = self.fc4(x) # get output vector, decoder\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss() # mean squared error loss function\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5) # root mean square prop\n",
    "#                                   |        |              |___________ Reduce learning rate after every few epoch for regulate the convergence\n",
    "#                                   |        |________ Learning rate\n",
    "#                                   |_________________ Get all parameters form sae object"
   ]
  },
  {
   "source": [
    "## Training the SAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1 loss: 1.771559476852417\n",
      "epoch: 2 loss: 1.0967310667037964\n",
      "epoch: 3 loss: 1.0534603595733643\n",
      "epoch: 4 loss: 1.0382895469665527\n",
      "epoch: 5 loss: 1.0308328866958618\n",
      "epoch: 6 loss: 1.0265097618103027\n",
      "epoch: 7 loss: 1.0236423015594482\n",
      "epoch: 8 loss: 1.02169930934906\n",
      "epoch: 9 loss: 1.0207910537719727\n",
      "epoch: 10 loss: 1.0193887948989868\n",
      "epoch: 11 loss: 1.0190292596817017\n",
      "epoch: 12 loss: 1.0183582305908203\n",
      "epoch: 13 loss: 1.0178768634796143\n",
      "epoch: 14 loss: 1.017232894897461\n",
      "epoch: 15 loss: 1.0171600580215454\n",
      "epoch: 16 loss: 1.016748309135437\n",
      "epoch: 17 loss: 1.0167073011398315\n",
      "epoch: 18 loss: 1.0162416696548462\n",
      "epoch: 19 loss: 1.0162694454193115\n",
      "epoch: 20 loss: 1.0159766674041748\n",
      "epoch: 21 loss: 1.016108512878418\n",
      "epoch: 22 loss: 1.0156826972961426\n",
      "epoch: 23 loss: 1.0156831741333008\n",
      "epoch: 24 loss: 1.0155222415924072\n",
      "epoch: 25 loss: 1.015668511390686\n",
      "epoch: 26 loss: 1.0153682231903076\n",
      "epoch: 27 loss: 1.015243411064148\n",
      "epoch: 28 loss: 1.014931321144104\n",
      "epoch: 29 loss: 1.0131922960281372\n",
      "epoch: 30 loss: 1.0115772485733032\n",
      "epoch: 31 loss: 1.0105546712875366\n",
      "epoch: 32 loss: 1.0080938339233398\n",
      "epoch: 33 loss: 1.0077165365219116\n",
      "epoch: 34 loss: 1.0036979913711548\n",
      "epoch: 35 loss: 1.003259539604187\n",
      "epoch: 36 loss: 1.0005934238433838\n",
      "epoch: 37 loss: 0.9998491406440735\n",
      "epoch: 38 loss: 0.9971268177032471\n",
      "epoch: 39 loss: 0.9962445497512817\n",
      "epoch: 40 loss: 0.9946491122245789\n",
      "epoch: 41 loss: 0.9939002990722656\n",
      "epoch: 42 loss: 0.9924528002738953\n",
      "epoch: 43 loss: 0.9917216300964355\n",
      "epoch: 44 loss: 0.9887269139289856\n",
      "epoch: 45 loss: 0.9900021553039551\n",
      "epoch: 46 loss: 0.9866835474967957\n",
      "epoch: 47 loss: 0.9901218414306641\n",
      "epoch: 48 loss: 0.9839051365852356\n",
      "epoch: 49 loss: 0.9810643196105957\n",
      "epoch: 50 loss: 0.9794471263885498\n",
      "epoch: 51 loss: 0.9779300093650818\n",
      "epoch: 52 loss: 0.975318968296051\n",
      "epoch: 53 loss: 0.9793845415115356\n",
      "epoch: 54 loss: 0.9749622941017151\n",
      "epoch: 55 loss: 0.9727713465690613\n",
      "epoch: 56 loss: 0.9710339307785034\n",
      "epoch: 57 loss: 0.9717823266983032\n",
      "epoch: 58 loss: 0.9707529544830322\n",
      "epoch: 59 loss: 0.9703981280326843\n",
      "epoch: 60 loss: 0.9671241641044617\n",
      "epoch: 61 loss: 0.9660807847976685\n",
      "epoch: 62 loss: 0.9637536406517029\n",
      "epoch: 63 loss: 0.963204562664032\n",
      "epoch: 64 loss: 0.9601427316665649\n",
      "epoch: 65 loss: 0.9590463638305664\n",
      "epoch: 66 loss: 0.9590080380439758\n",
      "epoch: 67 loss: 0.9561675190925598\n",
      "epoch: 68 loss: 0.9542437791824341\n",
      "epoch: 69 loss: 0.9537140727043152\n",
      "epoch: 70 loss: 0.9510166049003601\n",
      "epoch: 71 loss: 0.9525404572486877\n",
      "epoch: 72 loss: 0.9484923481941223\n",
      "epoch: 73 loss: 0.9497921466827393\n",
      "epoch: 74 loss: 0.9469698071479797\n",
      "epoch: 75 loss: 0.9467376470565796\n",
      "epoch: 76 loss: 0.9449156522750854\n",
      "epoch: 77 loss: 0.9451647400856018\n",
      "epoch: 78 loss: 0.9439752101898193\n",
      "epoch: 79 loss: 0.9435400366783142\n",
      "epoch: 80 loss: 0.9416473507881165\n",
      "epoch: 81 loss: 0.9425403475761414\n",
      "epoch: 82 loss: 0.9406237602233887\n",
      "epoch: 83 loss: 0.941092848777771\n",
      "epoch: 84 loss: 0.9396057724952698\n",
      "epoch: 85 loss: 0.939792811870575\n",
      "epoch: 86 loss: 0.9380196928977966\n",
      "epoch: 87 loss: 0.9382582902908325\n",
      "epoch: 88 loss: 0.9369000196456909\n",
      "epoch: 89 loss: 0.9377451539039612\n",
      "epoch: 90 loss: 0.9359709024429321\n",
      "epoch: 91 loss: 0.9367579817771912\n",
      "epoch: 92 loss: 0.9356215596199036\n",
      "epoch: 93 loss: 0.9358391761779785\n",
      "epoch: 94 loss: 0.9345583319664001\n",
      "epoch: 95 loss: 0.9350329637527466\n",
      "epoch: 96 loss: 0.9338980317115784\n",
      "epoch: 97 loss: 0.9343797564506531\n",
      "epoch: 98 loss: 0.9334331750869751\n",
      "epoch: 99 loss: 0.9338325262069702\n",
      "epoch: 100 loss: 0.9326239228248596\n",
      "epoch: 101 loss: 0.9331260919570923\n",
      "epoch: 102 loss: 0.9321362972259521\n",
      "epoch: 103 loss: 0.9324911236763\n",
      "epoch: 104 loss: 0.9315794110298157\n",
      "epoch: 105 loss: 0.9319191575050354\n",
      "epoch: 106 loss: 0.9310215711593628\n",
      "epoch: 107 loss: 0.9313943386077881\n",
      "epoch: 108 loss: 0.9301247596740723\n",
      "epoch: 109 loss: 0.9307718276977539\n",
      "epoch: 110 loss: 0.9298794269561768\n",
      "epoch: 111 loss: 0.9301452040672302\n",
      "epoch: 112 loss: 0.9292412996292114\n",
      "epoch: 113 loss: 0.9296194911003113\n",
      "epoch: 114 loss: 0.9285243153572083\n",
      "epoch: 115 loss: 0.9289955496788025\n",
      "epoch: 116 loss: 0.9281872510910034\n",
      "epoch: 117 loss: 0.9284394979476929\n",
      "epoch: 118 loss: 0.9275112748146057\n",
      "epoch: 119 loss: 0.9279087781906128\n",
      "epoch: 120 loss: 0.9270052909851074\n",
      "epoch: 121 loss: 0.9271792769432068\n",
      "epoch: 122 loss: 0.9262997508049011\n",
      "epoch: 123 loss: 0.9266693592071533\n",
      "epoch: 124 loss: 0.9257720112800598\n",
      "epoch: 125 loss: 0.9261466860771179\n",
      "epoch: 126 loss: 0.9252896308898926\n",
      "epoch: 127 loss: 0.9254629611968994\n",
      "epoch: 128 loss: 0.9247079491615295\n",
      "epoch: 129 loss: 0.9250888824462891\n",
      "epoch: 130 loss: 0.9240994453430176\n",
      "epoch: 131 loss: 0.9243806004524231\n",
      "epoch: 132 loss: 0.9236298203468323\n",
      "epoch: 133 loss: 0.9239391684532166\n",
      "epoch: 134 loss: 0.922998309135437\n",
      "epoch: 135 loss: 0.9234306812286377\n",
      "epoch: 136 loss: 0.9224048256874084\n",
      "epoch: 137 loss: 0.9228644371032715\n",
      "epoch: 138 loss: 0.9218841791152954\n",
      "epoch: 139 loss: 0.9223531484603882\n",
      "epoch: 140 loss: 0.9213111400604248\n",
      "epoch: 141 loss: 0.9218418598175049\n",
      "epoch: 142 loss: 0.920985221862793\n",
      "epoch: 143 loss: 0.9216789603233337\n",
      "epoch: 144 loss: 0.9206324219703674\n",
      "epoch: 145 loss: 0.9210459589958191\n",
      "epoch: 146 loss: 0.9199883341789246\n",
      "epoch: 147 loss: 0.9205654859542847\n",
      "epoch: 148 loss: 0.9196838140487671\n",
      "epoch: 149 loss: 0.9200253486633301\n",
      "epoch: 150 loss: 0.9192202091217041\n",
      "epoch: 151 loss: 0.9196606278419495\n",
      "epoch: 152 loss: 0.9187451004981995\n",
      "epoch: 153 loss: 0.9192391037940979\n",
      "epoch: 154 loss: 0.9182701110839844\n",
      "epoch: 155 loss: 0.9186629056930542\n",
      "epoch: 156 loss: 0.9177678823471069\n",
      "epoch: 157 loss: 0.9182794690132141\n",
      "epoch: 158 loss: 0.9176203608512878\n",
      "epoch: 159 loss: 0.9179272055625916\n",
      "epoch: 160 loss: 0.91717529296875\n",
      "epoch: 161 loss: 0.9174763560295105\n",
      "epoch: 162 loss: 0.9167388081550598\n",
      "epoch: 163 loss: 0.9170331954956055\n",
      "epoch: 164 loss: 0.9164179563522339\n",
      "epoch: 165 loss: 0.91680508852005\n",
      "epoch: 166 loss: 0.915981650352478\n",
      "epoch: 167 loss: 0.9160990715026855\n",
      "epoch: 168 loss: 0.9154517650604248\n",
      "epoch: 169 loss: 0.9158508777618408\n",
      "epoch: 170 loss: 0.9152253866195679\n",
      "epoch: 171 loss: 0.9156951904296875\n",
      "epoch: 172 loss: 0.9151015877723694\n",
      "epoch: 173 loss: 0.9152730703353882\n",
      "epoch: 174 loss: 0.9148863554000854\n",
      "epoch: 175 loss: 0.915072500705719\n",
      "epoch: 176 loss: 0.9146831631660461\n",
      "epoch: 177 loss: 0.9147503972053528\n",
      "epoch: 178 loss: 0.9142762422561646\n",
      "epoch: 179 loss: 0.9143959879875183\n",
      "epoch: 180 loss: 0.9139917492866516\n",
      "epoch: 181 loss: 0.9142066836357117\n",
      "epoch: 182 loss: 0.9138286709785461\n",
      "epoch: 183 loss: 0.9137126207351685\n",
      "epoch: 184 loss: 0.9135064482688904\n",
      "epoch: 185 loss: 0.9133762121200562\n",
      "epoch: 186 loss: 0.9131675958633423\n",
      "epoch: 187 loss: 0.9130028486251831\n",
      "epoch: 188 loss: 0.9128865003585815\n",
      "epoch: 189 loss: 0.9128332734107971\n",
      "epoch: 190 loss: 0.912677526473999\n",
      "epoch: 191 loss: 0.9126889109611511\n",
      "epoch: 192 loss: 0.9121633172035217\n",
      "epoch: 193 loss: 0.9125153422355652\n",
      "epoch: 194 loss: 0.9118695259094238\n",
      "epoch: 195 loss: 0.9118560552597046\n",
      "epoch: 196 loss: 0.9115738868713379\n",
      "epoch: 197 loss: 0.9116654396057129\n",
      "epoch: 198 loss: 0.9112941026687622\n",
      "epoch: 199 loss: 0.9111976027488708\n",
      "epoch: 200 loss: 0.9107970595359802\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0. # for normalize the train loss\n",
    "    for id_user in range(nb_users):\n",
    "        # pytorch need batch not vector\n",
    "        #           |\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        #                                            |______ put data into index zero of new dimension\n",
    "        target = input.clone() # keep the copy of input\n",
    "        if torch.sum(target.data > 0) > 0: # get users who at least rated one movie\n",
    "            output = sae(input) # sae.forward(input)\n",
    "            target.require_grad = False # do not compute gradiant with respect the target (for optimize)\n",
    "            output[target == 0] = 0 # replace movies that not rated with zeros of output vector (for optimize)\n",
    "            loss = criterion(output, target) # compute MSE loss\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # number of movies / number of movies have possitive (non zero) ratings\n",
    "            #                                       |                        |____ For avoid zero division error\n",
    "            #                                       |_____________________________ Get number of movies with possitive ratings\n",
    "            loss.backward() # In which direction need to update the weights (increase or decrease)\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector) # update the loss in each epoch\n",
    "            s += 1.\n",
    "            optimizer.step() # update the weights with RMSprop\n",
    "    print(f\"epoch: {epoch} loss: {train_loss/s}\")"
   ]
  },
  {
   "source": [
    "## Testing the SAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test loss: 0.9521654844284058\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "  input = Variable(training_set[id_user]).unsqueeze(0) # model predict according to input training data by users and give ratings for movies which are not rated by the user base on rated movies from training set\n",
    "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "  if torch.sum(target.data > 0) > 0:\n",
    "    output = sae(input) # predict base on training set data\n",
    "    target.require_grad = False\n",
    "    output[target == 0] = 0 # Compare only with rated movies on test set\n",
    "    loss = criterion(output, target)\n",
    "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "    s += 1.\n",
    "print(f\"test loss: {test_loss/s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}